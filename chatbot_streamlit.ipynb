{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suhail372/files_for_chatbot/blob/master/chatbot_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import streamlit as st\n",
        "\n",
        "class VectorSearchWrapper:\n",
        "    def __init__(self, location_is_hyd=False):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.EMBED_MODEL = 'sentence-transformers/paraphrase-MiniLM-L3-v2'\n",
        "        self.location_is_hyd = location_is_hyd\n",
        "\n",
        "        self.hyd_json_file_path = 'combined files/cleaned_and_combined_hyd.json'\n",
        "        self.blore_json_file_path = 'combined files/cleaned_and_combined_blore.json'\n",
        "\n",
        "        self.model = SentenceTransformer(self.EMBED_MODEL, device=self.device)\n",
        "        self.saved_vectors_path = 'saved_vectors'\n",
        "        self.index_hyd = None\n",
        "        self.index_blore = None\n",
        "        self.embeddings_hyd = []\n",
        "        self.embeddings_blore = []\n",
        "        self.id_to_entry = {}\n",
        "        self.run()\n",
        "\n",
        "    def embedding(self, text_data):\n",
        "        embedding = self.model.encode(text_data, convert_to_tensor=True, device=self.device)\n",
        "        normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=-1)\n",
        "        return normalized_embedding.cpu().numpy()\n",
        "\n",
        "    def preprocess_and_embed(self, json_file_path):\n",
        "        embedded_list = []\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            json_data = json.load(file)\n",
        "\n",
        "        for entry in json_data:\n",
        "            address = entry['Location']\n",
        "            terms = [term.strip() for term in address.split(',')]\n",
        "            replacable = ', '.join(terms[-4:]) if len(terms) > 4 else address\n",
        "\n",
        "            entry['text data'] = entry['text data'].replace(address, replacable)\n",
        "            text_data = entry[\"text data\"].replace(f'Name: {entry[\"Name\"]}', '')\n",
        "            entry_id = entry.get(\"Id\", None)\n",
        "\n",
        "            if entry_id is not None:\n",
        "                embedding = self.embedding(text_data)\n",
        "                embedded_list.append({\n",
        "                    \"embedding\": embedding,\n",
        "                    \"text\": text_data,\n",
        "                    \"id\": entry_id\n",
        "                })\n",
        "\n",
        "        return embedded_list\n",
        "\n",
        "    def save_embeddings(self, embeddings, location_name):\n",
        "        if not os.path.exists(self.saved_vectors_path):\n",
        "            os.makedirs(self.saved_vectors_path)\n",
        "\n",
        "        embeddings_array = np.vstack([entry[\"embedding\"] for entry in embeddings])\n",
        "        ids = [entry[\"id\"] for entry in embeddings]\n",
        "        texts = [entry[\"text\"] for entry in embeddings]\n",
        "\n",
        "        np.save(os.path.join(self.saved_vectors_path, f'embeddings_{location_name}.npy'), embeddings_array)\n",
        "        with open(os.path.join(self.saved_vectors_path, f'metadata_{location_name}.json'), 'w') as f:\n",
        "            json.dump({\"ids\": ids, \"texts\": texts}, f)\n",
        "\n",
        "    def load_embeddings(self, location_name):\n",
        "        embeddings_path = os.path.join(self.saved_vectors_path, f'embeddings_{location_name}.npy')\n",
        "        metadata_path = os.path.join(self.saved_vectors_path, f'metadata_{location_name}.json')\n",
        "\n",
        "        if os.path.exists(embeddings_path) and os.path.exists(metadata_path):\n",
        "            embeddings_array = np.load(embeddings_path)\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "\n",
        "            embeddings = [{\"embedding\": emb, \"text\": text, \"id\": id} for emb, text, id in zip(embeddings_array, metadata[\"texts\"], metadata[\"ids\"])]\n",
        "            return embeddings\n",
        "        return None\n",
        "\n",
        "    def create_faiss_index(self):\n",
        "        dimension = 384\n",
        "        return faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    def insert_data(self, index, embeddings):\n",
        "        embeddings_array = np.vstack([entry[\"embedding\"] for entry in embeddings])\n",
        "        index.add(embeddings_array)\n",
        "\n",
        "    def run(self):\n",
        "        self.embeddings_hyd = self.load_embeddings('hyd')\n",
        "        self.embeddings_blore = self.load_embeddings('blore')\n",
        "\n",
        "        if self.embeddings_hyd is None:\n",
        "            print(\"Hyderabad embeddings not found. Preprocessing and creating new embeddings.\")\n",
        "            self.embeddings_hyd = self.preprocess_and_embed(self.hyd_json_file_path)\n",
        "            self.save_embeddings(self.embeddings_hyd, 'hyd')\n",
        "        if self.embeddings_blore is None:\n",
        "            print(\"Bangalore embeddings not found. Preprocessing and creating new embeddings.\")\n",
        "            self.embeddings_blore = self.preprocess_and_embed(self.blore_json_file_path)\n",
        "            self.save_embeddings(self.embeddings_blore, 'blore')\n",
        "\n",
        "        self.index_hyd = self.create_faiss_index()\n",
        "        self.index_blore = self.create_faiss_index()\n",
        "\n",
        "        self.insert_data(self.index_hyd, self.embeddings_hyd)\n",
        "        self.insert_data(self.index_blore, self.embeddings_blore)\n",
        "\n",
        "    def search_faiss(self, query, k=3):\n",
        "        query_embedding = self.embedding(query).reshape(1, -1)\n",
        "        index = self.index_hyd if self.location_is_hyd else self.index_blore\n",
        "        embeddings = self.embeddings_hyd if self.location_is_hyd else self.embeddings_blore\n",
        "        json_file_path = self.hyd_json_file_path if self.location_is_hyd else self.blore_json_file_path\n",
        "\n",
        "        distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "        results = [{\"id\": embeddings[idx][\"id\"], \"text\": embeddings[idx][\"text\"]} for idx in indices[0]]\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        for i in results:\n",
        "            for j in data:\n",
        "                if i['id'] == j['Id']:\n",
        "                    i['text'] = j['text data']\n",
        "\n",
        "        return results\n",
        "\n",
        "class LLMHandler:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.llama_tokenizer, self.llama_model = self.load_llama_model()\n",
        "\n",
        "    def load_llama_model(self):\n",
        "        llama_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "        llama_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\").to(self.device)\n",
        "        llama_model.temperature = 1.0\n",
        "        llama_model.top_p = 0.95\n",
        "        return llama_tokenizer, llama_model\n",
        "\n",
        "    def generate_query(self, query, history):\n",
        "        input_prompt = f\"\"\"\n",
        "        <>\n",
        "            Generate a question similar to previous queries, and given user query, make it concise and do not generate an answer to the query.\n",
        "        <>\n",
        "        [INST]\n",
        "        History: {history}\n",
        "        Query: {query}\n",
        "        [/INST]\n",
        "        \"\"\"\n",
        "        inputs = self.llama_tokenizer(input_prompt, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        outputs = self.llama_model.generate(**inputs, max_new_tokens=1000, pad_token_id=self.llama_model.config.eos_token_id)\n",
        "        result_prompt = self.llama_tokenizer.batch_decode(outputs)[0]\n",
        "        return result_prompt.replace(input_prompt, \"\").strip()\n",
        "\n",
        "    def is_query(self, query):\n",
        "        query = query.lower()\n",
        "        my_list = [\"school\", \"schools\", \"facilities\", \"amenities\", \"sports\", \"faculty\", \"fees\", \"institute\", \"organisation\", \"org\", \"inst\", \"scl\", \"schol\"]\n",
        "        return any(item in query for item in my_list)\n",
        "\n",
        "    def requires_context(self, query):\n",
        "        query = query.lower()\n",
        "        context_keywords = [\"previous\", \"last\", \"before\", \"history\", \"context\", \"same\"]\n",
        "        return any(item in query for item in context_keywords)\n",
        "\n",
        "    def generate_chat_response(self, query, data):\n",
        "        input_prompt = f\"\"\"\n",
        "        <>You are a chatbot assistant that recommends schools to the user by describing the school's information. Generate an answer for the query given using the search results provided and nothing else in the response field.\n",
        "        <>\n",
        "        [INST]\n",
        "        User Query: {query}\n",
        "        Search results: {data}\n",
        "        [/INST]\n",
        "        \"\"\"\n",
        "        inputs = self.llama_tokenizer([input_prompt], return_tensors=\"pt\").to(self.device)\n",
        "        outputs = self.llama_model.generate(**inputs, max_new_tokens=1000, pad_token_id=self.llama_model.config.eos_token_id)\n",
        "        result_prompt = self.llama_tokenizer.decode(outputs[0])\n",
        "        return result_prompt.replace(input_prompt, \"\").strip()\n",
        "\n",
        "    def get_not_school_related_response(self):\n",
        "        not_school_related_responses = [\n",
        "            \"I'm sorry, but I can only provide information related to schools. How can I assist you with a school-related query today?\",\n",
        "            \"It looks like your question isn't related to schools. Could you please ask something about schools so I can help you better?\",\n",
        "            \"I'm here to help with school-related questions! If you have any queries about schools, feel free to ask.\",\n",
        "            \"Oops! I can only assist with questions about schools. Please let me know if there's anything you need to know about schools.\",\n",
        "            \"I'm not equipped to handle this query. Do you have any school-related queries?\",\n",
        "            \"I'm focused on answering school-related questions. Can you ask something about schools?\"\n",
        "        ]\n",
        "        return random.choice(not_school_related_responses)\n",
        "\n",
        "class Chatbot(VectorSearchWrapper, LLMHandler):\n",
        "    def __init__(self, location_is_hyd=False):\n",
        "        VectorSearchWrapper.__init__(self, location_is_hyd)\n",
        "        LLMHandler.__init__(self)\n",
        "\n",
        "    def process_query(self, user_query, user_history):\n",
        "        if self.is_query(user_query):\n",
        "            if self.requires_context(user_query):\n",
        "                user_query = self.generate_query(user_query, user_history)\n",
        "            search_results = self.search_faiss(user_query, k=3)\n",
        "            formatted_results = \"\\n\".join([f\"{result['text']}\" for result in search_results])\n",
        "            chat_response = self.generate_chat_response(user_query, formatted_results)\n",
        "            return chat_response\n",
        "        else:\n",
        "            return self.get_not_school_related_response()\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"School Information Chatbot\")\n",
        "\n",
        "st.sidebar.title(\"Choices sidebar\")\n",
        "choice = st.sidebar.radio(\"Choose location:\", (\"Hyderabad\", \"Bangalore\"))\n",
        "st.sidebar.write(f\"Current location: {choice}\")\n",
        "location_is_hyd = True if choice == \"Hyderabad\" else False\n",
        "\n",
        "# Initialize the chatbot once and store it in session state\n",
        "if \"chatbot\" not in st.session_state or st.session_state.chatbot.location_is_hyd != location_is_hyd:\n",
        "    st.session_state.chatbot = Chatbot(location_is_hyd)\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "prompt = st.chat_input(\"Enter your prompt\")\n",
        "if prompt:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    chatbot = st.session_state.chatbot\n",
        "    user_history = st.session_state.get(\"history\", \"\")\n",
        "    response = chatbot.process_query(prompt, user_history)\n",
        "    st.session_state.history = user_history + f\"\\n{prompt}\"\n",
        "\n",
        "    with st.spinner(\"Generating response...\"):\n",
        "        time.sleep(2)  # Simulate longer processing time\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "id": "sgqprtNFoHlw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}