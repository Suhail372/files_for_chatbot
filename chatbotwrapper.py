# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyKnWkHPw41dZd-nSXdTPYCY_JGbB6GG
"""

class ChatbotWrapper:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.repository_path = '/content/files_for_chatbot'
        self.EMBED_MODEL = 'sentence-transformers/all-mpnet-base-v2'
        self.DIM = 768
        self.collection = None
        self.collection_name = "chat_demo"
        self.MILVUS_URI = 'http://localhost:19530'
        [self.MILVUS_HOST, self.MILVUS_PORT] = self.MILVUS_URI.split('://')[1].split(':')
        self.llama_tokenizer=None
        self.llama_model=None
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    def initialize_model(self):
        # Load the pre-trained model and tokenizer
        model_name = self.EMBED_MODEL
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)

    def load_json_data(self):
        json_files = []
        for root, directories, files in os.walk(self.repository_path):
            for file in files:
                if file.endswith('.json'):
                    file_path = os.path.join(root, file)
                    json_files.append(file_path)
        return json_files
    def preprocess(self,json_data):
        # Convert each JSON entry into a single text string
        text_data = ""
        for key, value in json_data.items():
            if isinstance(value, list):
                # Convert list to string
                value_str = ', '.join(map(str, value))
                text_data += f"{key}: {value_str}~"
            else:
                text_data += f"{key}: {value}~"

        return text_data
    def embedding(self,text_data):

        inputs = self.tokenizer(text_data, return_tensors='pt', padding=True, truncation=True)
        inputs = {key: tensor.cuda() for key, tensor in inputs.items()}  # Move tensors to CUDA
        with torch.no_grad():
            # Forward pass through the model
            outputs = self.model(**inputs)

        embeddings = outputs.last_hidden_state.mean(dim=1)  # Assuming you want to use mean pooling

        # Normalize the embeddings if needed
        embeddings=embeddings[0]

        normalized_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)
        return normalized_embeddings

    def preprocess_and_embed(self, json_files):
        # Your preprocessing and embedding logic here
        embedded_data={}
        # Define the embedding pipeline
        model_name=self.EMBED_MODEL
        #]model_name = 'sentence-transformers/all-MiniLM-L6-v2'
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

        # Define the embedding pipeline
        self.model=self.model.to(self.device)

        # Load and process JSON data
        sumo=0
        sett=set()
        embedded_list=dict()

        print(json_files)

        for i in json_files:

            with open(i, 'r') as file:
                json_data = json.load(file)

            # Preprocess and embed each JSON entry
            sumo+=len(json_data)


            for entry in json_data:
                curr_dict={}
                text_data = self.preprocess(entry)
                if text_data not in sett:
                    sett.add(text_data)
                    embedded_list[text_data]=self.embedding(text_data)
        return embedded_list


    def create_collection(self,collection_name):
        connections.connect(host=self.MILVUS_HOST, port=self.MILVUS_PORT)

        has_collection = utility.has_collection(collection_name)

        if has_collection:
            utility.drop_collection(collection_name)
        # Create collection
        fields = [
            FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=self.DIM),
            FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=1000)
        ]
        schema = CollectionSchema(
            fields=fields,
            description="Towhee demo",
            enable_dynamic_field=True
        )
        collection = Collection(name=collection_name, schema=schema)

        # Change index here if you want to accelerate search
        index_params = {
            'metric_type': 'IP',
            'index_type': 'IVF_FLAT',
            'params': {'nlist': 4096,'nprobe':256}
        }
        collection.create_index(
            field_name='embedding',
            index_params=index_params
        )

        return collection

    def prepare_data(self,embedded_data):
        data = []
        for text_data, embedding in embedded_data.items():
            vector_id = str(uuid.uuid4())
            entity = {

                'embedding': embedding,
                'text': text_data
            }
            data.append(entity)
        return data
    def insert_data(self,embedded_data):
        self.collection = self.create_collection(self.collection_name)

        # Prepare data
        data_to_insert = self.prepare_data(embedded_data)

        # Insert data into the collection
        self.collection.insert(data_to_insert)
    def search_milvus(self,query):
        embedded_vec=self.embedding(query).cpu().numpy()
        self.collection.load()
        res=self.collection.search(
            data=[embedded_vec],
            anns_field="embedding",
            param={
            'metric_type': 'IP',
            'params': {'nlist': 4096}
                    },
            limit=3,

            output_fields=["text"]   )

        text_li=list()
        id_li=list()
        dist_li=list()
        for i, hits in enumerate(res):

            for hit in hits:
                id_li.append(hit.entity.id)
                dist_li.append(hit.entity.distance)
                text_li.append(hit.entity.get("text"))
        data=dict()
        data["id"]=id_li
        data["dist"]=dist_li
        data["text"]=text_li
        return data
    def load_llama(self):
        llama_model_name = "meta-llama/Llama-2-7b-chat-hf"
        token=userdata.get('HF_TOKEN')
        self.llama_tokenizer=AutoTokenizer.from_pretrained(llama_model_name,token=token,padding=True)
        self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token
        self.llama_model = AutoModelForCausalLM.from_pretrained(llama_model_name,token=token, low_cpu_mem_usage=True, torch_dtype='auto')
        self.llama_model.to(self.device)



    def generate_chat_response(self,query:str,data:dict)->str:
        search_results = data["text"]
        input_text = f"Query: {query}\nSearch Results: {search_results}"

        # Tokenize the input
        inputs = self.llama_tokenizer(input_text, return_tensors="pt", truncation=True, padding=True)
        inputs= {key: tensor.cuda() for key, tensor in inputs.items()}
        # Generate prompt
        prefix = "Generate a friendly response based on query and search results to the user"
        input_prompt = f"{prefix} {self.llama_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)}"
        inputs =self.llama_tokenizer(input_text, return_tensors="pt", truncation=True, padding=True)
        inputs= {key: tensor.cuda() for key, tensor in inputs.items()}
        output_ids = self.llama_model.generate(inputs['input_ids'])
        result_prompt = self.llama_tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return result_prompt
    def run(self):
        self.initialize_model()
        json_files=self.load_json_data()
        embeddings=self.preprocess_and_embed(json_files)
        self.insert_data(embeddings)
        self.load_llama()
    def search(self,query:str)->str:
        data=self.search_milvus(query)
        response=self.generate_chat_response(query,data)
        return response