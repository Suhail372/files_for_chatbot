{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540d9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from milvus import default_server\n",
    "from pymilvus import (\n",
    "    connections, utility, Collection,\n",
    "    CollectionSchema, FieldSchema, DataType\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c51b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchWrapper:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda:0\")  # Use GPU if CUDA is available\n",
    "\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.EMBED_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n",
    "        self.DIM = 768\n",
    "        self.json_file_path='F:\\\\Chat_Proj\\\\Project Files\\\\combining and automation\\\\cleaned_and_combined_hyd.json'\n",
    "        self.collection = None\n",
    "        self.collection_name = \"chat_demo\"\n",
    "        self.MILVUS_URI = 'http://localhost:19530'\n",
    "        [self.MILVUS_HOST, self.MILVUS_PORT] = self.MILVUS_URI.split('://')[1].split(':')\n",
    "        self.result=\"\"\n",
    "        self.embeddings=dict()\n",
    "        self.history=list()\n",
    "        self.run()\n",
    "    def initialize_model(self):\n",
    "        # Load the pre-trained model and tokenizer\n",
    "        model_name = self.EMBED_MODEL\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "    def embedding(self,text_data):\n",
    "\n",
    "        inputs = self.tokenizer(text_data, return_tensors='pt', padding=True, truncation=True)\n",
    "        if self.device.type == 'cuda':\n",
    "            inputs = {key: tensor.cuda() for key, tensor in inputs.items()}  # Move tensors to CUDA\n",
    "        if self.device.type == 'xla':\n",
    "            inputs = {key: tensor.to(self.device) for key, tensor in inputs.items()}# Move tensors to TPU\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Assuming you want to use mean pooling\n",
    "\n",
    "        # Normalize the embeddings if needed\n",
    "        embeddings=embeddings[0]\n",
    "\n",
    "        normalized_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "        return normalized_embeddings\n",
    "    \n",
    "    def preprocess_and_embed(self):\n",
    "        embedded_list = {}\n",
    "        with open(self.json_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Preprocess and embed each JSON entry\n",
    "        for entry in json_data:\n",
    "            curr_dict = {}\n",
    "            text_data = entry[\"text data\"]\n",
    "            entry_id = entry.get(\"Id\", None)  # Extract the ID from JSON (assuming it has an \"id\" field)\n",
    "            \n",
    "            if text_data not in embedded_list:  # Avoid duplicate embeddings\n",
    "                embedding = self.embedding(text_data)\n",
    "                embedded_list[entry_id] = {\n",
    "                    \"embedding\": embedding,\n",
    "                    \"text\": text_data\n",
    "                }\n",
    "\n",
    "        return embedded_list\n",
    "    def create_collection(self,collection_name):\n",
    "        connections.connect(host=self.MILVUS_HOST, port=self.MILVUS_PORT)\n",
    "\n",
    "        has_collection = utility.has_collection(collection_name)\n",
    "\n",
    "        if has_collection:\n",
    "            utility.drop_collection(collection_name)\n",
    "        # Create collection\n",
    "        fields = [\n",
    "            FieldSchema(name='id', dtype=DataType.INT64, is_primary=True,auto_id=False),\n",
    "            FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=self.DIM),\n",
    "            FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=1000)\n",
    "        ]\n",
    "        schema = CollectionSchema(\n",
    "            fields=fields,\n",
    "            description=\"Towhee demo\",\n",
    "            enable_dynamic_field=True\n",
    "        )\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "        # Change index here if you want to accelerate search\n",
    "        index_params = {\n",
    "            'metric_type': 'IP',\n",
    "            'index_type': 'IVF_FLAT',\n",
    "            'params': {'nlist': 4096,'nprobe':256}\n",
    "        }\n",
    "        collection.create_index(\n",
    "            field_name='embedding',\n",
    "            index_params=index_params\n",
    "        )\n",
    "\n",
    "        return collection\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        embedded_data = self.embeddings\n",
    "\n",
    "        for entry_id, entry in embedded_data.items():\n",
    "            entity = {\n",
    "                'id': entry_id,  # Use the provided ID\n",
    "                'embedding': entry['embedding'],\n",
    "                'text': entry['text']\n",
    "            }\n",
    "            data.append(entity)\n",
    "\n",
    "        return data\n",
    "    def insert_data(self,collection_name):\n",
    "        self.collection = self.create_collection(collection_name)\n",
    "\n",
    "        # Prepare data\n",
    "        data_to_insert = self.prepare_data()\n",
    "        # Insert data into the collection\n",
    "        self.collection.insert(data_to_insert)\n",
    "    def run(self,collection_name=None):\n",
    "        if collection_name is None:\n",
    "            collection_name = self.collection_name\n",
    "\n",
    "        # Initialize model asynchronously\n",
    "        self.initialize_model()\n",
    "        \n",
    "\n",
    "        # Preprocess and embed data asynchronously\n",
    "        self.embeddings =self.preprocess_and_embed()\n",
    "\n",
    "        # Insert data asynchronously\n",
    "        self.insert_data(collection_name)\n",
    "        \n",
    "        \n",
    "    def search_milvus(self,query):\n",
    "        embedded_vec=self.embedding(query).cpu().numpy()\n",
    "        collection=Collection(name=self.collection_name)\n",
    "        collection.load()\n",
    "        res=collection.search(\n",
    "            data=[embedded_vec],\n",
    "            anns_field=\"embedding\",\n",
    "            param={\n",
    "            'metric_type': 'IP',\n",
    "            'params': {'nlist': 4096}\n",
    "                    },\n",
    "            limit=3,\n",
    "\n",
    "            output_fields=[\"text\"]   )\n",
    "\n",
    "        text_li=list()\n",
    "        id_li=list()\n",
    "        dist_li=list()\n",
    "        for i, hits in enumerate(res):\n",
    "\n",
    "            for hit in hits:\n",
    "                id_li.append(hit.entity.id)\n",
    "                dist_li.append(hit.entity.distance)\n",
    "                text_li.append(hit.entity.get(\"text\"))\n",
    "        data=dict()\n",
    "        data[\"id\"]=id_li\n",
    "        data[\"dist\"]=dist_li\n",
    "        data[\"text\"]=text_li\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d14611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffad824",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot=VectorSearchWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f415a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=chatbot.search_milvus(query=\"CBSE schools  Location:dilsukhnagar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1ce61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [880, 1882, 1592], 'dist': [0.8021835088729858, 0.7947474718093872, 0.7850978374481201], 'text': ['Name: DILSUKHNAGAR PUBLIC SCHOOL~Category: Other~Location: BN Reddy Colony, Vanasthalipuram, 5-6-525, Rd Number 9/5, Sri Krishna Devaraya Nagar, B.N Reddy Nagar, Hyderabad, Telangana 500070, India~Faculty: ~Sports: ~Amenities: Transport~Board: CBSE~Years: 0~Fee: 34000~Since: Not Available~Strength: Not Available~', 'Name: DILSUKHNAGAR PUBLIC SCHOOL~Category: Other~Location: 8GVH+6XH, Dps school line, Sri Raghavendra Nagar Colony, Jyothi Nagar, Kharmanghat, Telangana 500079, India~Faculty: ~Sports: ~Amenities: Transport~Board: CBSE~Years: 0~Fee: 40000~Since: Not Available~Strength: Not Available~', 'Name: DILSUKHNAGAR PUBLIC SCHOOL~Category: Other~Location: Alkapuri-RK Puram Road, polkampally, Ramakrishnapuram, L. B. Nagar, Hyderabad, Telangana 500035, India~Faculty: ~Sports: ~Amenities: Transport~Board: State Board~Years: 0~Fee: 34000~Since: Not Available~Strength: Not Available~']}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c53f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_server.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
